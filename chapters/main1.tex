\chapter{Обзор средств и подходов для оптимизации сборки nutch}
\subsection*{Плагины nutch}
Система плагинов nutch подобна плагинам к Eclipse. Основная логика nutch описывается плагинами. Все что относится к разбору документов, индексации и поиску реализовано плагины. Подгружаемый модуль nutch предоставляет одно или более расширение так называемых точек расширения (extension-points). Один подгружаемый модуль может относиться к нескольким точкам расширения, так же как несколько подгружаемых моуелй могут относиться к одной и той же точке расширения. Каждая точка расширения предоставляет интерфейс, который необходимо реализовать в плагине. Для каждого плагина используется собственный загрузчик классов (class-loader), который в нужный момент загружает плагин в систему.\cite{nutchbib}
\subsection*{Ранжирование}
Поскольку в момент поступления url в систему мы сразу можем определить её полезность, возникает желание сначала скачивать именно ``полезные'' url. База ссылок nutch устроена таким образом что с каждым url связан ранг, который образуется с помощью плагина nutch ``scoring filter''.
Достаточно написать свой плагин, который бы увеличивал ранг полезных url. Поскольку плагин контролирует ранг ссылки на каждой стадии сборки, нет необходимости изменять код ядра nutch.

\subsubsection*{Ранжирование в nutch}
Как уже говорилось, ранжирование в nutch основано на плагинах. Плагин ранжирования является расширением точки ``ScoringFilter'' и реализует соответствующий интерфейс. ScoringFilter руководит определением ранга ссылки в момент попадания её в систему, модифицирует его после скачивания и разбора документа. Так же ScoringFilter отвечает непосредственно за генерацию ``sort value'' в момент выборки ссылок для скачивания.

Все плагины ранжирования организованы в цепь таким образом что полученный ``sort value'' одного плагина попадает на вход другого. В дальнейшем на основе ``score value'' формируется список скачиваемых страниц, при этом страницы с более высоким ``score value'' попадают в него раньше, чем страницы с меньшим (за создание списка отвечает набор работ generate в nutch). Порядок плагинов и количество url в списке определяется пользователем.\cite{nutchscore}

\subsubsection*{Информация о полезности url}
Необходимо выбрать способ, которым по url будет определяться её полезность. Поскольку можно использовать несколько плагинов в связке, достаточно определить полезность посредством задания ранга для ``полезных'' url, и для обычных, а различные промежуточные значения переложить на другие плагины.

Варианты задания полезных url:
\begin{enumerate}
 \item при помощи задания префикса
 \item при помощи задания регулярного выражения
\end{enumerate}
Хотя первый вариант будет работать быстрее и более прост, он не дает достаточного контроля, и высокий ранг будут получать лишние url, поэтому было решено использовать регулярные выражения.

Самый простой способ хранения регулярных выражений --- это добавить их в виде файла ресурса в конфигурацию nutch, но таким способом нельзя обновлять и изменять его в процессе сборки. Для этих целей было решено хранить все регулярные выражения в базе данных.
\subsection*{Генерация черных списков}
Основная задача черных списков --- уменьшение размера базы ссылок~---~crawldb.

\subsubsection*{Crawldb}
\label{sec:crawldb}
Одна из основных составляющих nutch --- база ссылок crawldb, которая представляет из себя записи вида $\langle url, crawldatum\rangle$, где $crawldatum$ это дополнительная информация об url, такая как:
\begin{itemize}
 \item текущее состояние (например оно показывает была ли ссылка скачана, или является ли данная ссылка редиректом на другую страницу)
 \item время когда документ по ссылке был в последний раз скачан
 \item время когда документ была в последний раз обновлен
 \item сколько раз система пыталась скачать данный документ
 \item как часто надо обновлять документ
 \item дополнительные произвольные данные 
\end{itemize}

Crawldb используется во многих задачах, время которое тратится на обработку crawldb линейно зависит от её размера. Время на выполнения всего цикла зависит от размера crawldb, и количества ссылок выбираемых для скачки. Хотя само время скачивания очень сильно зависит от канала и доменов с которых идет скачка, общее время можно представить в виде:
$$t_{c}=n_{c}*c_{1}+n_{g}*c_{2}$$ Где $n_{c}$ - число записей crawldb, $n_{g}$ - число выбираемых ссылок, а $t_{c}$ общее время цикла. В реальных условиях $c_{1}/c_{2}\approx 0.0013$, таким образом при $n_{c}=1000000$ и $n_{g}=20000$ на работу с crawldb тратится порядка 30\% времени. 

\subsubsection*{Фильтры}
Фильтрация ссылок в nutch реализована через плагины с точкой расширения URLFilter. Фильтры используются в момент добавления ссылки в базу ссылок и во время её обновления. В nutch реализовано несколько плагинов для фильтрации:
\begin{itemize}
 \item PrefixURLFilter --- фильтрация по префиксу url. Служит, как правило, для ограничения сборки определенными доменами.
 \item RegexURLFilter --- фильтрация по суффиксу url. Используются для ограничения форматов файлов.
 \item SuffixURLFilter --- фильтрация по регулярному выражению. 
\end{itemize}

Каждый из плагинов допускает ``пропускающие'' и ``исключающие'' правила, которые берутся из файлов конфигурации. 
Так как система должна работать непрерывно, а фильтры должны изменятся в ходе работы, необходимо изменить плагины работающие с префиксами и регулярными выражениями для работы с базой данных. Поскольку суффиксные фильтры нацелены на то, что бы отлавливать не поддерживающиеся форматы данных (.jpeg .css .zip) их можно не изменять.

\subsubsection*{Создание фильтров}
Фильтры ограничивающие домены и форматы тривиально создаются из списка доменов и расширений поддерживаемых форматов. Это дает самое общее ограничение области ссылок, при этом остается еще большое число ссылок не являющихся ``полезными''.
Например если мы хотим скачивать только новости то нам совершенно не интересно знать что на в том же домене располагается форум, или какие-либо статьи. Так же часто присутствует большое число технических ссылок, например некоторые сайты делают внешние ссылки как редиректы с собственного домена и.т.п.

Основную сложность для ручного создания фильтров представляют как раз технические разделы сайта, да и оценить в ручную размер любой другой части сайта не просто. Для того что бы эффективно находить подобные разделы необходима обратная связь от nutch в процессе работы. Так же возникает желание автоматически создавать фильтры.

\subsubsection*{Анализ crawldb}
Для анализа crawldb в nutch используется утилита readdb которая умеет получать статистику по базе. Наибольший интерес представляет такой предоставляемый ею набор метрик, как число ссылок определенного статуса, разбитые по доменам. Что бы стало возможным нахождение бесполезных разделов, необходимо реализовать возможность получения статистики не только по доменам, но и по крупным их частям. К сожалению это не возможно эффективно сделать лишь за счет плагинов --- необходимо изменять код ядра nutch. Так же, в отличии от стандартной утилиты, статистика должна поступать не на stdout а во внешнюю базу данных, для простоты дальнейшего анализа.

\subsubsection*{Автоматическая генерация фильтров}
Основная сложность в автоматической генерации --- создать достаточно эффективный метод с низкой вероятностью ошибок первого рода (отклонение разделов с полезной информации). Проблема заключается в том, что необходимо оставить не только все ``полезные ссылки'', но и те страницы без которых не все ``полезные'' ссылки достижимы из корневой страницы домена. 

Вернемся к примеру с lenta.ru. Все новости находятся по ссылкам вида \ref{eq:lentanews}, а архив новостей, без которого мы не сможем получить все новости, находится в разделе \ref{eq:lentaarch} который не должен попасть под фильтры.

\begin{equation}\label{eq:lentanews}
http://lenta\textbackslash.ru/news/\textbackslash d\{4\}/\textbackslash d\{2\}/\textbackslash d\{2\}/\textbackslash w+/
\end{equation}
\begin{equation}\label{eq:lentaarch}
 http://lenta\textbackslash.ru/\textbackslash d\{4\}/\textbackslash d\{2\}/\textbackslash d\{2\}/ 
\end{equation}

Формальные требования к автоматической генерации:
\begin{itemize}
 \item ни одна ``полезная'' ссылка  не должна попадать под фильтры
 \item после применения фильтров все ``полезные'' ссылки должны быть достижимы из корневой ссылки домена
 \item значительно число остальных ссылок должно попасть под фильтры
\end{itemize}

Т.к фильтры и статистика хранятся в базе данных, генератор фильтров может выступать как отдельное приложение.
\section*{Методы оценки эффективности}
Под эффективностью сборки далее будет пониматься отношение $n_{i}/t_{i}$, где $n_{i}$ --- число скаченных ссылок за итерацию $i$, а $t_{i}$ --- время итерации. Стоит заметить, что если для ранних итерациях основное время используется для обработки сегмента, то на поздних большую часть времени система занимается обработкой базы ссылок (crawldb) и базой обратных ссылок (linkdb).