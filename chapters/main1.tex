\chapter{Алгоритмы}
Подход к решению задачи:
\begin{enumerate}
 \item запустить систему без модификаций;
 \item найти самое слабое место в системе;
 \item принять меры по его устранению;
 \item перейти к пункту \textit{2}.
\end{enumerate}
\section{Ранжирование}
При работе системы в стандартной конфигурации было замечено, что только из 10\% скачиваемых веб-страниц выделяются документы. Это происходит из-за неоптимального упорядочивания ссылок из \textit{сrawldb}.

Определение порядка выбора URL для скачивания существенно сказывается на эффективности работы робота.\cite{crawl}\cite{focused}\cite{opic} Порядок неважен только в том случае, если робот нацелен на одноразовое скачивание всего Web, и нагрузка создаваемеая роботом на целевые сайты не важна, так как тогда каждая известная URL будет в конце концов загружена. Однако большинство роботов не способно посетить каждый URL по трем основным причинам:
\begin{itemize}
 \item Ограничение по ресурсам --- размер хранилища, ширина канала, CPU time для обработки страниц.
 \item Сбор документов занимает время, поэтому в определенный момент робот вынужден заново посещать некоторые страницы для нахождения изменений.
 \item Динамическое создание страниц --- сейчас большинство сайтов разадают не статический контент, а создают его динамически при помощи скриптов обрабатывающих URL и возвращающих результат, таким образом количество страниц на сайте может быть неограницено.
\end{itemize}

Во всех остальных случаях важно что бы робот сначала посещал ``важные'' страницы. Ранжирование отвечает за определенеие того, на сколько URL ``важна''.
В Nutch порядок выбора URL определяется в плагинах подключенных к точке расширения \textit{ScoringFilter}, в результате работы которых каждой URL сопоставляется \textit{score}. На каждой итерации для скачивания выбираются URL с наибольшим score.

В стандартной конфигурации Nutch для выбора ссылок используется \textit{OPIC Score} плагин.
\subsection{OPIC}
\textit{OPIC} --- On-Line Page Importance Computation\cite{opic}. Данный алгоритм расчитывает важность страницы на основе важности страниц на него ссылающихся. В отличии от off-line методов, когда сначала скачивается часть Web, а потом на основе полученного графа ссылок вычисляется важность страниц, этот метод позволяет работать когда большая часть сетевого графа еще не известна.

В Nutch реализована упрощенная версия данного алгоритма, при котором в момент скачивания страницы к score каждой из ссылок с нее(\textit{outlink}) добавляется $S_{0}/n$ где $S_{0}$ --- score скачанной страницы, а $n$ --- число исходящих ссылок. Данный способ плохо подходит для поиска новостей, поскольку для новостей основным признаком ``важности'' веб-страницы является присутствие в нем текста новости, что на практике не связано с количеством входящих ссылок.

Поскольку в системе идентификация новостей производится только по URL возникает желание скачивать только те URL которые подходят под правла. Однако 
