\section{Раннее удаление дубликатов}
Для выдачи качественных результатов поиска важно что бы в индексе не было дубликаов. Под дубликатом понимаются:
\begin{itemize}
 \item web-страницы с одинаковым URL;
 \item web-страницы с одинаковым текстом и доменом. 
\end{itemize}
\subsection{Реализация в Nutch}
В Nutch удаление дубликатов реализовано в виде серии MapReduce задач и работает достаточно быстро --- на дедубликацию как правило уходит не больше 3\% от общего времени. Однако у этого метода есть ряд существенных недостатков.
\begin{itemize}
 \item Дедубликация происходит уже после индексации документов. При этом дубликаты тоже индексируются, что занимает существенно большее время (порядка 10\% от общего времени цикла).
 \item Для эффективной работы MapReduce желательно что бы индекс хранился в распределенной файловой системе, а для быстрого поиска по индексу необходимо наличие копии в локальной файловой системе. Таким образом  нужно постоянно хранить согласованные копии индекса в двух файловых системах.
 \item Для удаления дубликатов достаточно знать только набор md5 хэшей и URL, а приходится читать весь индекс.
\end{itemize}
\subsection{Алгоритм}
Поскольку для удаления дубликатов достаточно небольшого количества данных (URL + md5 для каждого документа) было решено использовать Key Value базу данных. После получения текста статьи проверяется есть ли в базе данных такой хэш или URL, если есть, то страница отбрасывается, иначе в базу добавляются данные о новой странице и страница индексируется. Основным недостатком данного подхода является невозможность обновления документа (так как мы не изменяем уже готовые части индекса). В рамках данной системы это не критично, так как интересует появление новых новостей, а не обновление старых.

Поскольку сервер с базой данных должен справляться с нагрузкой с целого кластера был произведен анализ различных реализаций Key Value СУБД.
\subsection{Сравнение Key Value СУБД}
Были рассмотрены следующие реализации:
\begin{itemize}
 \item Memcached\footnote{\href{http://memcached.org/}{http://memcached.org/}} --- система кэширования данных разработанная для ускорения веб приложений. Изначально была создана для LiveJournal в 2003 году. Данные хранятся только в памяти, что позволяет осуществлять быстрый доступ, но при этом необходимо отдельно заботиться о сохранении данных на диск.
 \item MongoDB\footnote{\href{http://mongodb.org/}{http://mongodb.org/}} --- документо-ориентированная Key Value СУБД. MondoDB позиционирует себя как промежуточное звено между простейшими Key Value хранилищами и реляционными СУБД.
 \item Project Voldemort\footnote{\href{http://project-voldemort.com/}{http://project-voldemort.com/}} --- распределенное отказоустойчивое Key Value хранилище. Одним из приемуществ данной системы является отсутсвие единой точки отказа (single point of failure). Project Voldemort используется в качестве хранилища данных в LinkedIn\footnote{\href{http://www.linkedin.com/}{http://www.linkedin.com/}}.
 \item Tokyo Cabinet\footnote{\href{http://fallabs.com/tokyocabinet/}{http://fallabs.com/tokyocabinet/}} --- встроенное Key Value хранилище. Поддерживает как хранение данных только в памяти, так и на диске. Для удаленного доступа используется Tokyo Tyrant\footnote{\href{http://fallabs.com/tokyotyrant/}{http://fallabs.com/tokyotyrant/}}. Протокол Tokyo Tyrant почти полностью совместим с Memcached.
\end{itemize}

Сравнение данных СУБД представлено в таблице \ref{tab:kv}.

\begin{table}[h]
\caption{\label{tab:kv}Сравнение Key Value СУБД.}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Название & Memcached & MongoDb & Project Voldemort & Tokyo Cabinet\\
\hline
Операции put/ms & 4.03 & 6.65 & 1.21 & 3.25 \\
\hline
Операции get/ms & 4.54 & 3.05 & 1.01 & 4.39 \\
\hline
Устойчивость & - & + & + & + \\
\hline
Распределенность & + & + & + & - \\
\hline
Репликация & - & + & + & - \\
\hline
Модель данных & bin & object & object & bin\\
\hline
\end{tabular}
\end{center}
\end{table}

\subsubsection{Измерение производительности}
Так как время выполнения put и get запросов является критическим для нашей системы, было решено разработать утилиту\footnote{\href{http://github.com/volkov/kvstorage-test}{http://github.com/volkov/kvstorage-test}} измеряющую производительность различных СУБД на данных, с которыми потом будет осуществляться работа. Результаты данных тестов представлены в строчках put и get таблицы \ref{tab:kv}.

Тестирование происходило по следующему сценарию:
\begin{enumerate}
 \item в базу добавляются все URL из рабочего индекса (5688210 ссылок);
 \item проверяется наличие всех URL из индекса по одному из сегметнов (52583 ссылок).
\end{enumerate}
Добавление и проверка производятся в 10 потоков. Сервер и клиент находятся на разных хостах ($1.2$ GHz 2007 Xeon, 1.7GB RAM, скорость последовательного доступа к диску 50MB/s), время ping между хостами --- 0.6ms.
\subsubsection{MongoDB}
В качестве Key Value СУБД была выбрана MongoDB так как она проста в настройке, достаточно надежна и производительна. MongoDB поддерживает несколько \textit{баз данных} на одном сервере (аналог \textit{database} в реляционных СУБД), а так же различные \textit{коллекции} (аналог таблиц) внутри одной базы данных. Из недостатков можно отметить отсутвие какой-либо авторизации и аутентификации, но поскольку кластер работает в DMZ это не существенно.                                                                                                                                                                                                                                                                                        
\section{Особенности реализации}
Данная функциональность была реализована как плагин с точкой расширения \textit{IndexingFilter}. Плагины этой точки расширения, как правило, применяются для добавления полей к индексируемому документу, но так же подходят и для отсечения ненужных документов.

Данные в MongoDB хрянятся в коллекциях \textit{url} и \textit{hash}, в которых хранятся записи вида $\langle url,segmentid,taskid \rangle$ и $\langle md5,segmentid,taskid \rangle$ соответственно.
\textit{Segmentid} и \textit{Taskid} хранятся для того, что бы определить ситуацию когда Hadoop по различным причинам перезапустил часть задачи (это просиходит при некорректном завершении частей задач, или когда определенная часть задачи выполняется слишком медленно). В таком случае, несмотря на наличие записи в MongoDB, фильтр пропускает документ. Фильтр работает следующим образом:
\begin{enumerate}
 \item если в MongoDB нет ни записи с соответствующим $url$, ни записи с соответствующим $md5$, они добавляются и документ пропускается;
 \item если в MongoDB есть запись с данным $url$ и $md5$, и $taskid$ в них совпадает с текущей, документ пропускается;
 \item во всех остальных случаях документ фильтруется.  
\end{enumerate}

В случае некорректного завершения всей задачи, все записи с текущим $segmentid$ удаляются.

\section{Результаты}
Время на проверку документа в базе данных практически не сказалось на времени индексации, поскольку индексация в основном загружает процессор и производится в несколько потоков. Так что во время ожидания ответа от MongoDB обсчитывается другой документ. При этом необходим дополнительный сервер с базой данных, который может быть объединен с сервером осуществляющим поиск или любым другим.

Время которое экономится за счет того, что дубликаты не индексируются, напрямую зависит от количества дубликатов в выборке. При использовании системы в реальных условиях данных подход позволяет экономить до 44\% на этапе индексации.

Поскольку операция проверки и добавления не атомарна, при параллельном доступе к базе возникает ситуация, когда дубликаты все же попадают в индекс, однако, как показало тестирование, в созданном таким образом индексе дублируется меньше 0.01\% документов и этим можно принебречь.

\section{Черные списки}
При долгой работе сборки значительно увеличивается размер базы ссылок crawldb.

Crawldb используется практически на всех этапах, время которое тратится на обработку crawldb линейно зависит от её размера. Время на выполнения всего цикла зависит от размера crawldb и количества ссылок выбираемых для скачки. Хотя само время скачивания очень сильно зависит от канала и сайтов с которых идет скачка, его можно представить в виде:
$$t_{c}=n_{c}*c_{1}+n_{g}*c_{2}$$ Где $n_{c}$ - число записей crawldb, $n_{g}$ - число выбираемых ссылок, а $t_{c}$ общее время цикла. В реальных условиях $c_{1}/c_{2}\approx 0.0013$, таким образом при $n_{c}=1000000$ и $n_{g}=20000$ на работу с crawldb тратится порядка 30\% времени. 

Значительная часть ссылок хранимых в базе не представляют никакого интереса. Например, не имеет смысла хранить ссылки на документы неподдерживаемого формата (*.mp3, *.avi, *.jpg), или все ссылки на раздел с форумом. За отсечение ненужных URL в Nutch отвечает система URL фильтров.

\subsection{Фильтры}
Фильтрация ссылок в Nutch реализована через плагины с точкой расширения URLFilter. Фильтры используются в момент добавления ссылки в базу ссылок и во время её обновления. В Nutch реализовано несколько плагинов для фильтрации:
\begin{itemize}
 \item PrefixURLFilter --- фильтрация по префиксу URL. Служит, как правило, для ограничения сборки определенными доменами.
 \item SuffixURLFilter --- фильтрация по суффиксу URL. Используются для ограничения форматов файлов.
 \item RegexURLFilter --- фильтрация по регулярному выражению. 
\end{itemize}

Каждый из плагинов допускает ``пропускающие'' и ``исключающие'' правила, которые берутся из файлов конфигурации. 
% Так как система должна работать непрерывно, а фильтры должны изменятся в ходе работы, необходимо изменить плагины работающие с префиксами и регулярными выражениями для работы с базой данных. Поскольку суффиксные фильтры нацелены на то, что бы отлавливать не поддерживающиеся форматы данных (.jpeg .css .zip) их можно не изменять.

\subsection{Создание фильтров}
Фильтры ограничивающие домены и форматы тривиально создаются из списка доменов и расширений поддерживаемых форматов. Это дает самое общее ограничение области ссылок, при этом еще остается большое число ссылок не являющихся ``полезными''.
Например, если мы хотим скачивать только новости, то нам совершенно не интересно знать что на в том же домене располагается форум, или какие-либо статьи. Так же часто присутствует большое число технических ссылок, например некоторые сайты делают внешние ссылки как редиректы с собственного домена. Основную сложность для ручного создания фильтров представляют как раз технические разделы сайта.

Необходимо создать метод с низкой вероятностью ошибок первого рода (отклонение разделов с полезной информации), эффективно отсекающий нежелательные URL. Проблема заключается в том, что необходимо оставить не только все ``полезные ссылки'', но и те страницы без которых не все ``полезные'' ссылки достижимы из корневой страницы домена. 

Рассмотрим в качестве примера сайт lenta.ru. Все новости находятся по ссылкам вида \ref{eq:lentanews}, а архив новостей, без которого мы не сможем получить все новости, находится в разделе \ref{eq:lentaarch} который не должен попасть под фильтры.

\begin{equation}\label{eq:lentanews}
http://lenta\textbackslash.ru/news/\textbackslash d\{4\}/\textbackslash d\{2\}/\textbackslash d\{2\}/\textbackslash w+/
\end{equation}
\begin{equation}\label{eq:lentaarch}
 http://lenta\textbackslash.ru/\textbackslash d\{4\}/\textbackslash d\{2\}/\textbackslash d\{2\}/ 
\end{equation}

Формальные требования к автоматической генерации:
\begin{itemize}
 \item ни одна ``полезная'' ссылка  не должна попадать под фильтры;
 \item после применения фильтров все ``полезные'' ссылки должны быть достижимы из корневой ссылки домена;
 \item значительное число остальных ссылок должно попасть под фильтры.
\end{itemize}

Задачу построения фильтров можно разбить на две части:
\begin{itemize}
 \item получение из crawldb данных в удобной для анализа форме;
 \item непосредственно анализ выделенных данных.
\end{itemize}

\subsection{Получение статистики по crawldb}
Для анализа crawldb в nutch используется утилита readdb которая умеет получать статистику по базе. Наибольший интерес представляет такой предоставляемый ею набор метрик, как число ссылок определенного статуса, разбитые по доменам. Что бы стало возможным нахождение бесполезных разделов, необходимо реализовать возможность получения статистики не только по доменам, но и по крупным их частям.

Основная идея заключается в том, что бы получить записи вида $\langle domain,prefix,metrics \rangle$ где:
\begin{itemize}
 \item domain --- домен к которому относится запись.
 \item prefix --- префикс url к которому относится запись.
 \item metrics --- набор пар вида $\langle state, count\rangle$, где $count$ --- это число документов с данным префиксом на данном домене, в состоянии $state$ (например: новая ссылка, скачанная ссылка, ``полезная ссылка'').
\end{itemize}
По подобным записям в дальнейшем достаточно легко делать выводы о разделах ресурсов.
\subsubsection{Алгоритм}
Поскольку данная задача работает с crawldb, размер которой может быть очень большим, алгоритм необходимо представить в виде MapReduce задачи.

\paragraph{Map}
На этапе map по $url$ и $crawldatum$ получается множество пар $\langle prefix,info\rangle+$, где $prefix$ это префикс url вместе с доменом, а $info$ это пара вида $\langle state,1\rangle$
$$\langle url, crawldatum \rangle \rightarrow \langle prefix,info\rangle+ $$

Сначала из $url$ неким способом получается множество префиксов $S_{p}$, например:
$$ http://lenta.ru/2010/05/09/ \rightarrow $$
$$ http://lenta.ru/2010/05/09, $$
$$ http://lenta.ru/2010/05/, $$
$$ http://lenta.ru/2010/, $$
$$ http://lenta.ru/; $$


Далее в зависимости от свойств $url$ описанных в $crawldatum$ создается множество метрик $S_{m}$, например если ссылка была ``полезной'' но не была еще скачана, то $$ S_{m}=\{\langle fit,1\rangle, \langle unfetched,1\rangle\}$$

Затем все пары из $S_{p} \times S_{m}$ попадают в выходной поток. Перед этапом reduce промежуточные записи сортируются, и важно что бы их было не слишком много, то есть $url$ следует разбивать на наименьшее число префиксов, но так, что бы основные разделы домена могли быть сопоставлены какому-нибудь префиксу, а множество метрик содержало только одно значение (это верно если $url$ не может находится сразу в нескольких состояниях).
\paragraph{Reduce}
На этапе reduce метрики префиксов суммируются и отбрасываются незначительные префиксы
$$\langle prefix,\langle state,1\rangle+\rangle \rightarrow \langle prefix,metricvector\rangle$$

$Metricvector$ представляет собой вектор из пар $\langle state, nurl \rangle$, где $nurl$ --- число $url$ c состоянием $state$.
Незначительными признаются метрики где сумма $nurl$ достаточно мала. Такие префиксы отбрасываются, а остальные сохраняются в базу данных.

\subsection{Реализация}
В ходе реализации был изменен класс CrawlDbReader, отвечающий за получение статистики, написаны классы для map и reduce.

\subsubsection{CrawlDbExtendedStatMapper}
CrawlDbExtendedStatMapper --- класс имплементирующий стандартный интерфейс Hadoop --- Mapper, который служит для создания собственных этапов map.
Для разбиения url на префиксы был использован UrlSplitter, который разбивал url на не более чем $n_{s}$ префиксов по символу ``/'', так же как и в примере с lenta.ru. Таким образом $|S_{p}|\leqslant n_{s}$.
В качестве метрик использовались следующие показатели:
\begin{itemize}
 \item unfetched ---документ по ссылке не был скачан;
 \item fetched --- документ по ссылке была скачана;
 \item fit --- ссылка была признана ``полезной''.
\end{itemize}
Первые два показателя брались непосредственно из статуса $crawldatum$, а последняя из мета данных, которые были проставлены с помощью \nameref{sec:scoringmeta}. Таким образом $|S_{m}|\leqslant2$. В результате, число записей попадающих в выходной поток $n\leqslant 2\cdot n_{s}\cdot n_{url}$, где $n_{url}$ --- число записей в crawldb.

\subsubsection{CrawlDbExtendedStatReducer}
CrawlDbExtendedStatReducer --- класс имплементирующий стандартный интерфейс Hadoop --- Reducer, который служит для создания собственных этапов reduce.

На этапе reduce суммируются все показатели для префикса и если $n_{unfetched}+n_{fetched} < n_{s}$ префикс отбрасывается. Для совместимости со стандартным CrawlDbReader'ом и уменьшения нагрузки на базу данных, префиксы не сразу добавляются в базу данных, а выводятся в файл. 
\subsubsection{Добавление в базу данных}
После отработки MapReduce работы, её результат считывается из файла, после чего все записи добавляются в базу данных одним запросом. Так же в базу передается время в которое статистика была создана.

Получать статистику можно достаточно редко --- например один раз в 5-20 циклов, таким образом, время на сбор статистики не оказывает существенного влияния на эффективность.
\subsection{Результаты}
Для crawldb c 10 000 000 url время получения статистики составляет порядка 15\% от общего времени работы цикла (для скачивания выбирается 20 000 ссылок). Таким образом, при создании статистики каждый двадцатый цикл, потеря времени составляет порядка 0.75\%, которой  можно пренебречь.

\section{Генерация фильтров}
При создании алгоритма были сделаны следующие предположения:
\begin{itemize}
 \item все ``полезные'' можно ограничить некотором числом разделов, не зависящем от числа документов. (под разделом понимается некий префикс url)
 \item архив документов находится в определенном разделе (под архивом понимаются документы с ссылками на ``полезные'' документы)
 \item архив достижим из корневого документа
 \item документов архива меньше чем ``полезных''
\end{itemize}

\subsection{Алгоритм}
Сам алгоритм, пользуясь некоторой эвристикой, признает некоторые префиксы ненужными, из которых потом создаются фильтры.
Алгоритм выбирающий префиксы согласно сделанным предположениям выглядит так:
\begin{enumerate}
 \item Выбирается вся статистика для конкретного домена.
 \item Домены для которых $u_{d}<n_{d}$ далее не рассматриваются.
 \begin{itemize}
  \item $u_{d}$ --- число ``полезных'' ссылок во всем домене
  \item $n_{d}$ --- пороговое значение, введенное для того, что бы не начать создавать фильтры до того, как будут получены ссылки на архив.
 \end{itemize}

 \item По префиксам для которых $u_{p}=0$ и $s_{p}>u_{d}$ создаются исключающие правила.
 \begin{itemize}
  \item $u_{p}$ --- число ``полезных'' ссылок с данным префиксом
  \item $s_{p}$ --- общее число известных ссылок с данным префиксом
 \end{itemize}
 Сам по себе префикс уже является префиксным фильтром.
\end{enumerate}


\subsection{Реализация}
Данная функциональность была реализована в виде отдельного приложения запускающегося сразу после создания статистики. Поскольку пользовательские фильтры были организованы на базе RegexURLFilter, для простоты управления автоматически созданные фильтры были тоже реализованы регулярными выражениями. 

Была предусмотрена возможность отключения конкретного фильтра таким образом, что бы он не создавался заново. Для этого фильтр не удаляется из базы данных, а становится не активен.

\subsection{Результат}
Качество результатов работы данного алгоритма достаточно сложно оценить, однако на тестовых данных автоматические фильтры достаточно успешно находили бесполезные разделы. Предполагается что существенное увеличение производительности будет получено на позднем этапе сборки - когда большая часть ``полезных'' документов уже будет скачена.
