\section{Черные списки}
При долгой работе сборки значительно увеличивается размер базы ссылок crawldb.

Crawldb используется практически на всех этапах, время которое тратится на обработку crawldb линейно зависит от её размера. Время на выполнения всего цикла зависит от размера crawldb, и количества ссылок выбираемых для скачки. Хотя само время скачивания очень сильно зависит от канала и доменов с которых идет скачка, общее время можно представить в виде:
$$t_{c}=n_{c}*c_{1}+n_{g}*c_{2}$$ Где $n_{c}$ - число записей crawldb, $n_{g}$ - число выбираемых ссылок, а $t_{c}$ общее время цикла. В реальных условиях $c_{1}/c_{2}\approx 0.0013$, таким образом при $n_{c}=1000000$ и $n_{g}=20000$ на работу с crawldb тратится порядка 30\% времени. 

Значительная часть ссылок хранимых в базе не представляют никакого интереса. Например, не имеет смысла хранить ссылки на документы неподдерживаемого формата (*.mp3, *.avi, *.jpg), или все ссылки на раздел с форумом. За отсечение ненужных URL в Nutch отвечает система URL фильтров.

\subsection{Фильтры}
Фильтрация ссылок в Nutch реализована через плагины с точкой расширения URLFilter. Фильтры используются в момент добавления ссылки в базу ссылок и во время её обновления. В Nutch реализовано несколько плагинов для фильтрации:
\begin{itemize}
 \item PrefixURLFilter --- фильтрация по префиксу URL. Служит, как правило, для ограничения сборки определенными доменами.
 \item SuffixURLFilter --- фильтрация по суффиксу URL. Используются для ограничения форматов файлов.
 \item RegexURLFilter --- фильтрация по регулярному выражению. 
\end{itemize}

Каждый из плагинов допускает ``пропускающие'' и ``исключающие'' правила, которые берутся из файлов конфигурации. 
% Так как система должна работать непрерывно, а фильтры должны изменятся в ходе работы, необходимо изменить плагины работающие с префиксами и регулярными выражениями для работы с базой данных. Поскольку суффиксные фильтры нацелены на то, что бы отлавливать не поддерживающиеся форматы данных (.jpeg .css .zip) их можно не изменять.

\subsection{Создание фильтров}
Фильтры ограничивающие домены и форматы тривиально создаются из списка доменов и расширений поддерживаемых форматов. Это дает самое общее ограничение области ссылок, при этом остается еще большое число ссылок не являющихся ``полезными''.
Например, если мы хотим скачивать только новости, то нам совершенно не интересно знать что на в том же домене располагается форум, или какие-либо статьи. Так же часто присутствует большое число технических ссылок, например некоторые сайты делают внешние ссылки как редиректы с собственного домена и.т.п.

Основную сложность для ручного создания фильтров представляют как раз технические разделы сайта, да и оценить в ручную размер любой другой части сайта не просто. Для того что бы эффективно находить подобные разделы необходима обратная связь от nutch в процессе работы. Так же возникает желание автоматически создавать фильтры.

\subsection{Анализ crawldb}
Для анализа crawldb в nutch используется утилита readdb которая умеет получать статистику по базе. Наибольший интерес представляет такой предоставляемый ею набор метрик, как число ссылок определенного статуса, разбитые по доменам. Что бы стало возможным нахождение бесполезных разделов, необходимо реализовать возможность получения статистики не только по доменам, но и по крупным их частям. К сожалению это не возможно эффективно сделать лишь за счет плагинов --- необходимо изменять код ядра nutch. Так же, в отличии от стандартной утилиты, статистика должна поступать не на stdout а во внешнюю базу данных, для простоты дальнейшего анализа.

\subsection{Автоматическая генерация фильтров}
Основная сложность в автоматической генерации --- создать достаточно эффективный метод с низкой вероятностью ошибок первого рода (отклонение разделов с полезной информации). Проблема заключается в том, что необходимо оставить не только все ``полезные ссылки'', но и те страницы без которых не все ``полезные'' ссылки достижимы из корневой страницы домена. 

Вернемся к примеру с lenta.ru. Все новости находятся по ссылкам вида \ref{eq:lentanews}, а архив новостей, без которого мы не сможем получить все новости, находится в разделе \ref{eq:lentaarch} который не должен попасть под фильтры.

\begin{equation}\label{eq:lentanews}
http://lenta\textbackslash.ru/news/\textbackslash d\{4\}/\textbackslash d\{2\}/\textbackslash d\{2\}/\textbackslash w+/
\end{equation}
\begin{equation}\label{eq:lentaarch}
 http://lenta\textbackslash.ru/\textbackslash d\{4\}/\textbackslash d\{2\}/\textbackslash d\{2\}/ 
\end{equation}

Формальные требования к автоматической генерации:
\begin{itemize}
 \item ни одна ``полезная'' ссылка  не должна попадать под фильтры
 \item после применения фильтров все ``полезные'' ссылки должны быть достижимы из корневой ссылки домена
 \item значительно число остальных ссылок должно попасть под фильтры
\end{itemize}

Т.к фильтры и статистика хранятся в базе данных, генератор фильтров может выступать как отдельное приложение.
\section{Получение статистики}
Основная идея заключается в том, что бы получить записи вида $\langle domain,prefix,metrics \rangle$ где:
\begin{itemize}
 \item domain --- домен к которому относится запись.
 \item prefix --- префикс url к которому относится запись.
 \item metrics --- набор пар вида $\langle state, count\rangle$, где $count$ --- это число документов с данным префиксом на данном домене, в состоянии $state$ (например: новая ссылка, скачанная ссылка, ``полезная ссылка'').
\end{itemize}
По подобным записям в дальнейшем достаточно легко делать выводы о разделах ресурсов.
\subsection{Алгоритм}
Поскольку данная задача работает с crawldb, размер которой может быть очень большим, алгоритм необходимо представить в виде  \nameref{sec:mapred} задачи.

\subsubsection{Map}
На этапе map по $url$ и $crawldatum$ получается множество пар $\langle prefix,info\rangle+$, где $prefix$ это префикс url вместе с доменом, а $info$ это пара вида $\langle state,1\rangle$
$$\langle url, crawldatum \rangle \rightarrow \langle prefix,info\rangle+ $$

Сначала из $url$ неким способом получается множество префиксов $S_{p}$, например:
$$ http://lenta.ru/2010/05/09/ \rightarrow $$
$$ http://lenta.ru/2010/05/09, $$
$$ http://lenta.ru/2010/05/, $$
$$ http://lenta.ru/2010/, $$
$$ http://lenta.ru/; $$


Далее в зависимости от свойств $url$ описанных в $crawldatum$ создается множество метрик $S_{m}$, например если ссылка была ``полезной'' но не была еще скачана, то $$ S_{m}=\{\langle fit,1\rangle, \langle unfetched,1\rangle\}$$

Затем все пары из $S_{p} \times S_{m}$ попадают в выходной поток. Для эффективной работы необходимо, что бы $url$ разбивалось на наименьшее число префиксов, но так, что бы основные разделы домена могли быть сопоставлены какому-нибудь префиксу. В идеале множество метрик может содержать только одно значение, если $url$ не может находится сразу в нескольких состояниях.
\subsubsection{Reduce}
На этапе reduce метрики префиксов суммируются и отбрасываются незначительные префиксы
$$\langle prefix,\langle state,1\rangle+\rangle \rightarrow \langle prefix,metricvector\rangle$$

$Metricvector$ представляет из себя вектор из пар $\langle state, nurl \rangle$, где $nurl$ --- число $url$ c состоянием $state$.
Незначительными признаются метрики где сумма $nurl$ достаточно мала. Такие префиксы отбрасываются, а остальные сохраняются в базу данных.

\subsection{Реализация}
В ходе реализации был изменен класс CrawlDbReader, отвечающий за получение статистики. Написаны классы для map и reduce.

\subsubsection{CrawlDbExtendedStatMapper}
CrawlDbExtendedStatMapper --- класс имплементирующий стандартный интерфейс \nameref{sec:hadoop} --- Mapper, который служит для создания собственных этапов map.
Для разбиения url на префиксы был использован UrlSplitter, который разбивал url на не более чем $n_{s}$ префиксов по символу ``/'', так же как и в примере с lenta.ru. Таким образом $|S_{p}|\leqslant n_{s}$ Пожалуй это самая простая реализация которая требует доработки в будущем.
В качестве метрик использовались следующие показатели:
\begin{itemize}
 \item unfetched ---документ по ссылке не был скачан.
 \item fetched --- документ по ссылке была скачана.
 \item fit --- ссылка была признана ``полезной''
\end{itemize}
Первые два показателя брались непосредственно из статуса $crawldatum$, а последняя из мета данных, которые были проставлены с помощью \nameref{sec:scoringmeta}. Таким образом $|S_{m}|\leqslant2$. В результате число записей попадающих в выходной поток $n\leqslant 2\cdot n_{s}\cdot n_{url}$, где $n_{url}$ --- число записей в crawldb.

\subsubsection{CrawlDbExtendedStatReducer}
CrawlDbExtendedStatReducer --- класс имплементирующий стандартный интерфейс \nameref{sec:hadoop} --- Reducer, который служит для создания собственных этапов reduce.

На этапе reduce суммируются все показатели для префикса и если $n_{unfetched}+n_{fetched} < n_{s}$ префикс отбрасывается. Для совместимости со стандартным CrawlDbReader'ом и уменьшения нагрузки на базу данных, префиксы не сразу добавляются в базу данных, а выводятся в файл. 
\subsubsection{Добавление в базу данных}
После отработки MapReduce работы, её результат считывается из файла, после чего все записи добавляются в базу данных одним запросом. Так же в базу передается время в которое статистика была создана.

Получать статистику можно достаточно редко --- например один раз в 5-20 циклов, таким образом, время на сбор статистики не оказывает существенного влияния на эффективность.
\subsection*{Результаты}
Для crawldb c 10 000 000 url время получения статистики составляет порядка 15\% от общего времени работы цикла с 20 000 ссылок. Таким образом, при создании статистики каждый двадцатый цикл, потеря времени составляет порядка 0.75\%, которой  можно пренебречь.

\section{Генерация фильтров}
При создании алгоритма были сделаны следующие предположения:
\begin{itemize}
 \item все ``полезные'' можно ограничить некотором числом разделов, не зависящем от числа документов. (под разделом понимается некий префикс url)
 \item архив документов находится в определенном разделе (под архивом понимаются документы с ссылками на ``полезные'' документы)
 \item архив достижим из корневого документа
 \item документов архива меньше чем ``полезных''
\end{itemize}

\subsection{Алгоритм}
Сам алгоритм, пользуясь некоторой эвристикой, признает некоторые префиксы ненужными, из которых потом создаются фильтры.
Простейший алгоритм выбирающий префиксы согласно сделанным предположениям выглядит так:
\begin{enumerate}
 \item Выбирается вся статистика для конкретного домена.
 \item Домены для которых $u_{d}<n_{d}$ далее не рассматриваются.
 \begin{itemize}
  \item $u_{d}$ --- число ``полезных'' ссылок во всем домене
  \item $n_{d}$ --- пороговое значение, введенное для того, что бы не начать создавать фильтры до того, как будут получены ссылки на архив.
 \end{itemize}

 \item По префиксам для которых $u_{p}=0$ и $s_{p}>u_{d}$ создаются исключающие правила.
 \begin{itemize}
  \item $u_{p}$ --- число ``полезных'' ссылок с данным префиксом
  \item $s_{p}$ --- общее число известных ссылок с данным префиксом
 \end{itemize}
 Сам по себе префикс уже представляет из себя префиксный фильтр.
\end{enumerate}


\subsection{Реализация}
Данная функциональность была реализована в виде отдельного приложения запускающегося сразу после создания статистики. Поскольку пользовательские фильтры были организованы на базе RegexURLFilter, для простоты управления автоматически созданные фильтры были тоже реализованы регулярными выражениями. 

Была предусмотрена возможность отключения конкретного фильтра таким образом, что бы он не создавался заново. Для этого фильтр не удалялся из базы данных, а просто становился не активен.

\subsection{Результат}
Качество результатов работы данного алгоритма достаточно сложно оценить, однако на тестовых данных автоматические фильтры достаточно успешно находили бесполезные разделы. Предполагается что существенное увеличение производительности будет получено на позднем этапе сборки - когда большая часть ``полезных'' документов уже будет скачена.
