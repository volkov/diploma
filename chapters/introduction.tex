\chapter{Введение} 
\epigraph{— В то время, когда наши корабли бороздят просторы
Вселенной…}{} На текущем этапе развития, когда общество осуществляет переход от
постиндустриальной эпохи к информационной, требования к системам хранения и
обработки информации непрерывно растут. Традиционные подходы не справляются с
ростом количетва данных. Трудно оценить общий объем данных, однако, по оценкам
IDC (International Data Corporation) в данный момент хранится порядка
$1.8\cdot10^{21}$ байт, что в 10 раз больше чем в 2006 году.

К значительному количеству данных можно получить доступ через Всемирную Паутину
(www). При таких объемах остро стоит задача организации эффективного поиска. Уже
в 2009 году Google Search обработал более 109,5 миллионов сайтов, и более
$10^{12}$ уникальных URL. На данный момент их индекс содержит $4\cdot10^{10}$ документов.

Одной из специфических областей поиска является поиск по новостным ресурсам.
 Для документов с новостных сайтов характерна привязка к дате, региону и тематике.
 Таким образом такие документы легко классифицировать, что позволяет производить более качественный поиск и анализ.
 В качественном инструменте для анализа СМИ заинтереснованы различные консалтинговые и pr агенства, пресс-службы, маркетинговые отделы крупных компаний.

Одна из задач поисковой системы - нахождение и загрузка документов(Web crawling), за которую отвечает поисковый робот(Spider, Crawler).
 Web crawling весьма ресурсоемкий процесс. Основные проблемы связаны с большим количеством данных,
 остутсвием  контроля над данными, постоянным изменением структуры ресурсов, динамическим созданием страниц и низким качеством некоторых ресурсов.
 Однако, специализация на определенной узкой области web позволяет существенно повысить производительность web crawler'а.

Конечной целью работы является создание системы способной эффективно индексировать новости в рунете.

\chapter{Обзор области}
\section{Web Search} 
\textbf{Поисковая система} --- система, разработанная для
поиска информации в www. Результаты поиска которой, как правило, представлены в
виде списка ``попадений''\fixme{Не понятно что такое попадения}. Информация может состоять из веб страниц,  изображений,
мультимедийной информации. Одной из первых поисковых систем стал
проект Archie, разработанный в 1990 году студентами McGill University. Программа скачивала
списки файлов с открытых ftp серверов, и добавляла их в базу с возможностью
поиска по названию. %Дальнейшим 

Поисковая система состоит из трех основных компонент: 
\begin{itemize}
 \item поисковый робот --- программа, предназначенная для перебора докуметнов и
занесения данных о них в базу. 
 \item индексатор --- программа, создающая на основе полученных с помощью робота данных индекс.
 \item поисковик --- программа, осуществляющая поиск в полученном индексе на основе поискового запроса.
\end{itemize} В условиях постоянно расширяющегося и изменяющегося www, непрерывно
возрастают требования к поисковым системам. 

\paragraph{Системы общего поиска} нацелены на охват большей части данных
доступных в www. Такие системы предназначены для поиска наиболее релевантных
документов относящихся к объекту поиска. 
\paragraph{Системы тематического поиска} более разнообразны, и требования к ним более специфичны.
 Например Google Microblogging Search Engine, ориентированный на поиск по записям в микроблогах,
где крайне важна задержка между созданием записи, и ее попадением в индекс.

\section{Поиск по новостям}

Основные источники новостей в www --- это электронные СМИ и блоги. По данным
liveinternet на 2008 год, рунет насчитывает 4392 сайта СМИ,
 а число блогов значительно больше - по данным Яндекс за 2009 год в русскоязычной блогосфере насчитывается порядка 840000 активных блогов, 
на которых ежедневно публикуется порядка 300000 постов.\footnote{http://mediarevolution.ru/audience/1962.html}
\fixme{Дополнить данными по нашему проекту. Посмотреть, сколько мы считываем URL'ов по RSS}
Очевидно, за прошедшее время количество таких сайтов значительно увеличилось. За сутки каждое
из подобный изданий публикует до 100 документов(lenta.ru). Таким образом, 
можно говорить о десятках миллионов создаваемых документов в год.

Под новостью понимается документ содержащий текст, заголовок и дату. Для СМИ и
блогов характерно:
\begin{itemize} 
 \item большое количество посторонних страниц, не содержащих новостей;
 \item схожая структура (как именования url, так и самого html);
 \item наличие rss ленты.
\end{itemize}

К новостным поисковым системам предъявляются следущие требования:
\begin{itemize} 
\item минимальное время между публикацией статьи на новостном ресурсе и ее 
    предоставление в поисковой выдаче;
\item поик должен осуществлять не по всей HTML--странице, а только по ее 
    существенным частям. 
\end{itemize}

\section{Задача}
Конечной целью работы является создание системы способной эффективно индексировать новости в рунете.

\fixme{перенести в конец. Оставить, просто изменить время (см п 1)}
\section{Результаты}
\begin{itemize}
 \item Сделан сравнительный анализ различных open source поисковые роботы (DataparkSearch,
AppSeek, mnlGoSearch, Nutch, Hounder, Heritix) и выбрать наиболее подходящий для
решения задачи
 \item Изменено поведение ядра nutch для более эффективной работы с индексом большого объема.
 \item Проанализированы различные key-value хранилища (Memcached, MongoDb, Project Voldemort, Tokyo Cabinet) и выбрано MongoDb в качестве хранилища для системы удаления дубликатов из индекса
 \item Разработан и реализован плагин к Nutch для раннего удаления дубликатов
 \item Разработан и реализован плагин для более эффективного ранжирования ссылок для новостных сайтов
 \item Разработана и реализована система для автоматического создания url фильтров
 \item Измененная система протестирована на реальных данных.
\end{itemize}

\chapter{Обзор средств}
Большинство популярных поисковых сервисов предоставляют возможность поиска по новостям (google, yandex, yahoo!), однако они пользуются закрытыми алгоритмами и не предоставляют доступа непосредственно к индексу
\section{Сравнение open source поисковых роботов}
Существует достаточно много open source поисковых роботов. Для успешного решения задачи робот должен справлятся с нагрузкой (порядка 100000 документов в день, база ссылок порядка $10^{9}$ и порядка $10^7$ документов в индексе), быть легко изменяем и расширяем. Поскольку предполагается коммерческое использование робота не протяжении долгого времени, проект должен быть достаточно зрелым и развивающимся.
\paragraph{Метрики}
\begin{itemize}
 \item язык
 \item поддержка robots.txt
 \item распределенность системы
 \item тип хранения индекса
 \item тип хранилища url
 \item поддержка
\end{itemize}
\paragraph{Роботы}
\begin{itemize}
 \item DataparkSearch --- поисковая система разработаная для поиска по локальным файлам, группам сайтов и интранету
 \item AspSeek --- поисковая система оптимизированная для работы с многими сайтами, и средней загрузкой --- до нескольких миллионов страниц
%  \item mnoGoSearch --- yet another crawler in C
 \item Nutch --- поисковая система основанная на Lucene
 \item Hounder --- поисковая система онованная на nutch
 \item Heritix --- еще одна java поисковая система
\end{itemize}
\paragraph{Сравнение}
\begin{table}[h]
\caption{\label{tab:crawlers}Сравнение поисковых роботов.}
\begin{center}
\begin{turn}{90}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Название & Язык & Распределенность & robots.txt & Индекс & Хранилище url & Количество документов\\
\hline
DataparkSearch & C & ~ & + & SQL database/собственный формат & SQL database & $10^{6}$\\
\hline
AspSeek & C++ & ?? & + & SQL database & SQL database & $10^{6}$\\
\hline
Nutch & Java & + & + & Lucene index & распределенный файл & $10^{9}$\\
\hline
Hounder & Java & + & + & Lucene index & распределенный файл & ???\\
\hline
\end{tabular}
\end{turn}
\end{center}
\end{table}


\subsection{Описание роботов}
\paragraph{DataparkSearch}\footnote{http://www.dataparksearch.org/} --- предназначен для работы с небольшой группой сайтов или интранета, написан на C. Состоит из двух частей --- индексатора и CGI фронтенда. DataparkSearch отделился в 2003 году от mnoGoSearch. Имеет встроенные парсеры для html, xml, есть возможность написания собственных парсеров для других форматов. Данные по ссылкам хрянятся в SQL базе данных. Можно запустить сразу несколько процессов индексации работающих с одной базой. Данные по документам могут храниться как в бд, так и в собственном формате на диске (cache mode), который эффективно работает с несколькими миллионами документов.
\paragraph{AspSeek}\footnote{http://www.aspseek.org/} --- поисковая система написанная на C++ и оптимизированная для работы с множеством сайтов. Состоит из индексирующего робота, поискового демона и CGI фронтенда. Данные поискового сервера хранятся в SQL базе данных и бинарных файлах (delta files), рассчитан для работы с несколькими миллионами документов.
\paragraph{Nutch}\footnote{http://nutch.apache.org/} --- поисковый робот написанный на java, работающий поверх системы Hadoop\footnote{http://hadoop.apache.org/}. Изначально Nutch разрабатывался в рамках проекта Lucene\footnote{http://lucene.apache.org/}, однако в 2005 году отделился как отдельный проект. Благодаря работе поверх Hadoop обладает хорошей маштабируемостью (до 100 машин в кластере). Nutch отличается гибкой системой плагинов, через которые осуществляется поддрежка множества протоколов (http, ftp, file) и форматов (от html до msexcel и swf).
\paragraph{Hounder}\footnote{http://hounder.org/} --- поисковая система на java, робот которой основан на Nutch. Из дополнительного функционала следует отметить фильтр Байеса для разбиения документов по категориям.

\subsection{Выбор}
В качестве основы системы был выбран Nutch, так как он полностью удовлетворяет требованиям:
\begin{itemize}
 \item нагрузка --- Nutch использовался в качестве основы для Sapphire Web Crawler\footnote{http://boston.lti.cs.cmu.edu/crawler/index.html}, с помощью которого было скачано более $10^{9}$ документов со средней скростью в 431 документ в секунду.
 \item расширямеость --- благодаря модульности и гибкой системе плагинов можно достаточно легко изменять поведение системы.
 \item поддержка --- проект разрабатывается более 7 лет, текущая стабильная версия проекта 1.2 была выпущена в сентябре 2010. Проект поддерживается ``Yahoo! Research Labs''.
\end{itemize}

\section{Архитектура Nutch}
Высокая масштабируемость робота достигается за счет работы поверх MapReduce фреймворка hadoop.
\paragraph{MapReduce} --- модель программирования для обработки больших объемов данных, впревые опубликованная\cite{googlemr} Google в 2004 году. При данном подходе логика программы реализуется в функциях \textit{map}, которая преобразует пары ключ/значение в набор промежуточных пар ключ/значение, и \textit{reduce}, которая обрабатывает все значения связанные с одним промежуточным ключом \ref{eq:mapred}.
\begin{equation}\label{eq:mapred}
\begin{split}
map:\langle key_{in}, value_{in}\rangle\rightarrow\langle key_{int}, value_{int}\rangle^{*} \\
reduce:\langle key_{int}, value_{int}^{+}\rangle\rightarrow\langle key_{out}, value_{out}\rangle^{*}
\end{split}
\end{equation}
Написанная таким образом программа может автоматически параллельно выполняться на кластере машин, программное обеспечение которых брало бы на себя распределение данных, управление выполнением задач, поддержку отказом и управление взаимодействием между узлами кластера. 

