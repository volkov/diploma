\chapter*{Введение}
\addcontentsline{toc}{chapter}{Введение} 
\epigraph{— В то время, когда наши корабли бороздят просторы
Вселенной…}{} На текущем этапе развития, когда общество осуществляет переход от
постиндустриальной эпохи к информационной, требования к системам хранения и
обработки информации непрерывно растут. Традиционные решения не справляются с
ростом количества данных. Трудно оценить общий объем данных, однако, по оценкам
IDC (International Data Corporation) в цифровом виде на данный момент хранится порядка
$1,8\cdot10^{21}$ байт, что в 10 раз больше чем в 2006 году.

К значительному количеству данных можно получить доступ через Всемирную Паутину
(WWW). При таких объемах остро стоит задача организации эффективного поиска. Уже
в 2009 году Google Search обработал более 109,5 миллионов сайтов, и более
$10^{12}$ уникальных URL. На данный момент их индекс содержит $4\cdot10^{10}$ документов.

Одной из специфических областей поиска является поиск по новостным ресурсам.
 Для документов с новостных сайтов характерна привязка к дате, региону и тематике.
 Таким образом, такие документы легко классифицировать, что позволяет производить более качественный поиск и анализ.
 В качественном инструменте для анализа СМИ заинтересованы различные консалтинговые и PR агентства, пресс-службы, маркетинговые отделы крупных компаний.

Одна из задач поисковой системы - нахождение и загрузка документов (Web crawling), за которую отвечает поисковый робот (Spider, Crawler).
 Web crawling весьма ресурсоемкий процесс. Основные проблемы связаны с большим количеством данных,
 отсутствием  контроля над данными, постоянным изменением структуры ресурсов, динамическим созданием страниц и низким качеством некоторых ресурсов.
 Однако, специализация на определенной узкой области web позволяет существенно повысить производительность web crawler'а.

Конечной целью работы является создание системы способной эффективно индексировать новости в рунете.

\chapter{Обзор области}
\section{Поисковые системы}
\textbf{Поисковая система} --- система, разработанная для
поиска информации в WWW. Результаты поиска которой, как правило, представлены в
виде списка ``попаданий''\fixme{Не понятно что такое попадания}. Информация может состоять из веб страниц,  изображений,
мультимедийных данных. 

Поисковая система состоит из трех основных компонент: 
\begin{itemize}
 \item поисковый робот --- программа, предназначенная для перебора документов и
занесения данных о них в базу;
 \item индексатор --- программа, создающая на основе полученных с помощью робота данных индекс;
 \item поисковик --- программа, осуществляющая поиск в полученном индексе на основе поискового запроса.
\end{itemize} В условиях постоянно расширяющегося и изменяющегося WWW, непрерывно
возрастают требования к поисковым системам. 

\paragraph{Системы общего поиска} нацелены на охват большей части данных
доступных в WWW. Такие системы предназначены для поиска наиболее релевантных
документов относящихся к объекту поиска. 
\paragraph{Системы тематического поиска} более разнообразны, и требования к ним более специфичны.
 Например, Google Microblogging Search Engine ориентирован на поиск по записям в микроблогах,
где крайне важна задержка между созданием записи, и ее попадением в индекс.

\section{Поиск по новостям}

Основные источники новостей в WWW --- это электронные СМИ и блоги. По данным
liveinternet на 2008 год, рунет насчитывает 4392 сайта СМИ,
 а число блогов значительно больше --- по данным Яндекс за 2009 год в русскоязычной блогосфере насчитывается порядка 840000 активных блогов, 
на которых ежедневно публикуется порядка 300000 постов.\footnote{\href{http://mediarevolution.ru/audience/1962.html}{http://mediarevolution.ru/audience/1962.html}}
\fixme{Дополнить данными по нашему проекту. Посмотреть, сколько мы считываем URL'ов по RSS}
Очевидно, за прошедшее время количество таких сайтов значительно увеличилось. За сутки каждое
из подобный изданий публикует до 100 документов (lenta.ru). Таким образом, 
можно говорить о десятках миллионов создаваемых документов в год.

Под новостью понимается документ содержащий текст, заголовок и дату. Для СМИ и
блогов характерно:
\begin{itemize} 
 \item большое количество посторонних страниц, не содержащих новостей;
 \item схожая структура (как именования URL, так и самого HTML);
 \item наличие RSS или другой новостной ленты (web feed).
\end{itemize}

К новостным поисковым системам предъявляются следующие требования:
\begin{itemize} 
\item минимальное время между публикацией статьи на новостном ресурсе и ее 
    предоставлением в поисковой выдаче;
\item поиск должен осуществлять не по всей HTML-странице, а только по ее 
    существенным частям. 
\end{itemize}

\chapter{Постановка Задачи}
Конечной целью работы является создание поискового робота способного эффективно индексировать новости в рунете.

\fixme{куда вообще воткнуть еще это определение?}
\textit{Поисковый робот} (Web crawler) --- программа для поиска веб-страниц в сети\cite{crawl}. Грубо говоря поисковый робот начинает с URL для начальной страницы $p_{0}$.
 Он скачивает $p_{0}$, выделяет все URL, которые в ней находятся, и добавляет их в очередь URL (\textit{crawling frontier}). Затем робот в некотором порядке выбирает URL из очереди и повторяет процесс.

Каждая скачанная страница передается клиенту, который затем создает индекс по страницам.
\section{Условия}
Важным фактором, влияющим на качество поиска, является идентификация страницы содержащей новость
 и выделение её содержательной части. В данной работе предполагается наличие базы данных,
 содержащей правила на основе регулярных выражений, которые по URL определяют содержит ли данная страница новость,
 а так же правил, по которым из веб-страницы выделяется содержательная часть.

Далее под \textit{документом} понимаются выделенные по этим правилам данные.

\section{Требования}
\begin{itemize}
 \item поддержка десятков миллионов документов;
 \item скорость роста базы документов --- более 50 тысяч документов в день;
 \item попадение в индекс документов из RSS лент не позже чем через 12 часов после их публикации;
 \item попадение в индекс старых документов из архива --- некоторые документы могут находиться достаточно ``глубоко`` (например чтобы получить новости месячной давности на ресурсе fontanka.ru необходимо сделать 5 переходов).
 \item отсутствие дубликатов в пределах одного домена --- под дубликатом понимается документ у которого совпадает URL или текст с другим документом.
\end{itemize}

В качестве основного показателя эффективности используется количество полученных документов за сутки.
 Поскольку объем данных непрерывно увеличивается, неизбежна деградация производительности. Это связано в первую очередь с постоянным ростом размера базы ссылок.
 Поэтому также в качестве метрики используется произведение количества полученных за сутки документов с общим числом документов. То есть если количество вычислительных мощностей растет прямо пропорционально числу документов в индексе, скорость получения новых документов должна хотя бы не падать, что подразумевает хорошую горизонтальную масштабируемость всех ключевых компонент системы.



\chapter{Обзор средств}
Большинство популярных поисковых сервисов предоставляют возможность поиска по новостям (Google, Yandex, Yahoo!), однако, они пользуются закрытыми алгоритмами и не предоставляют доступа непосредственно к индексу
\section{Сравнение open source поисковых роботов}
Существует достаточно много поисковых роботов разрабатывающихся под свободными лицензиями. Для успешного решения задачи робот должен справляться с нагрузкой (50000 документов в день, база ссылок порядка $10^{9}$ и порядка $10^7$ документов в индексе), быть легко изменяем и расширяем. Поскольку предполагается коммерческое использование робота не протяжении долгого времени, проект должен быть достаточно зрелым и развивающимся.
\paragraph{Метрики}
\begin{itemize}
 \item язык
 \item поддержка robots.txt
 \item распределенность системы
 \item тип хранения индекса
 \item тип хранилища URL
 \item поддержка
\end{itemize}
\paragraph{Роботы}
\begin{itemize}
 \item DataparkSearch --- поисковая система разработанная для поиска по локальным файлам, группам сайтов и интранету.
 \item AspSeek --- поисковая система оптимизированная для работы с многими сайтами, и средней загрузкой --- до нескольких миллионов страниц.
%  \item mnoGoSearch --- yet another crawler in C
 \item Nutch --- поисковая система основанная на Lucene.
 \item Hounder --- поисковая система основанная на Nutch.
%  \item Heritix --- еще одна java поисковая система
\end{itemize}

Сравнение данных поисковых роботов представлено в таблице \ref{tab:crawlers}
\begin{table}[h]
\caption{\label{tab:crawlers}Сравнение поисковых роботов.}
\begin{center}
% \begin{turn}{90}
\begin{tabular}{|c|c|c|c|c|c|c|}

\hline
Название & DataparkSearch & AspSeek & Nutch & Hounder \\
\hline
Язык & C & C++ & Java & Java \\
\hline
Распределенность & частичная & - & + & + \\
\hline
robots.txt & + & + & + & + \\
\hline
Индекс & RDBMS & RDBMS & Lucene & Lucene \\
\hline
Хранилище URL & RDBMS & RDBMS & файл & файл \\
\hline
Документы & $10^{6}$ & $10^{6}$ & $10^{9}$ & $10^{9}$ \\
\hline

% 
% \hline
% Название & Язык & Распределенность & robots.txt & Индекс & Хранилище url & Документы\\
% \hline
% DataparkSearch & C & ~ & + & SQL database/собственный формат & SQL database & $10^{6}$\\
% \hline
% AspSeek & C++ & ?? & + & SQL database & SQL database & $10^{6}$\\
% \hline
% Nutch & Java & + & + & Lucene index & распределенный файл & $10^{9}$\\
% \hline
% Hounder & Java & + & + & Lucene index & распределенный файл & ???\\
% \hline
\end{tabular}
% \end{turn}
\end{center}
\end{table}


\subsection{Описание роботов}
\paragraph{DataparkSearch}\footnote{\href{http://www.dataparksearch.org/}{http://www.dataparksearch.org/}} --- предназначен для работы с небольшой группой сайтов или интранета, написан на языке C. Состоит из двух частей --- индексатора и CGI фронтенда. DataparkSearch отделился в 2003 году от MnoGoSearch. Имеет встроенные парсеры для HTML, XML, есть возможность написания собственных парсеров для других форматов. Данные по ссылкам хранятся в SQL базе данных. Можно запустить сразу несколько процессов индексации работающих с одной базой. Данные по документам могут храниться как в базе данных, так и в собственном формате на диске (cache mode), который эффективно работает с несколькими миллионами документов.
\paragraph{AspSeek}\footnote{\href{http://www.aspseek.org/}{http://www.aspseek.org/}} --- поисковая система написанная на C++ и оптимизированная для работы с множеством сайтов. Состоит из индексирующего робота, поискового демона и CGI фронтенда. Данные поискового сервера хранятся в SQL базе данных и бинарных файлах (delta files), рассчитан для работы с несколькими миллионами документов.
\paragraph{Nutch}\footnote{\href{http://nutch.apache.org/}{http://nutch.apache.org/}} --- поисковый робот написанный на Java, работающий поверх системы Hadoop\footnote{\href{http://hadoop.apache.org/}{http://hadoop.apache.org/}}. Изначально Nutch разрабатывался в рамках проекта Lucene\footnote{\href{http://lucene.apache.org/}{http://lucene.apache.org/}}, однако в 2005 году отделился как отдельный проект. Благодаря работе поверх Hadoop, обладает хорошей маштабируемостью (до 100 машин в кластере). Nutch отличается гибкой системой плагинов, через которые осуществляется поддержка множества протоколов (HTTP, FTP, file) и форматов (от HTML до ms-excel и swf).
\paragraph{Hounder}\footnote{\href{http://hounder.org/}{http://hounder.org/}} --- поисковая система на Java, робот которой основан на Nutch. Из дополнительного функционала следует отметить фильтр Байеса для разбиения документов по категориям.

\subsection{Выбор}
В качестве основы системы был выбран Nutch, так как он полностью удовлетворяет требованиям.
\begin{itemize}
 \item Нагрузка --- Nutch использовался в качестве основы для Sapphire Web Crawler\footnote{\href{http://boston.lti.cs.cmu.edu/crawler/index.html}{http://boston.lti.cs.cmu.edu/crawler/index.html}}, с помощью которого было скачано более $10^{9}$ документов со средней скоростью в 431 документ в секунду.
 \item Расширяемость --- благодаря модульности и гибкой системе плагинов можно достаточно легко изменять поведение системы.
 \item Поддержка --- проект разрабатывается более 7 лет, текущая стабильная версия проекта 1.2 была выпущена в сентябре 2010. Проект поддерживается ``Yahoo! Research Labs''.
\end{itemize}

\section{Архитектура Nutch}
Высокая масштабируемость робота достигается за счет работы поверх MapReduce фреймворка Hadoop\cite{hadoopdefguide}. Hadoop на данный момент представляет набор подпроектов Apache Software Foundation, среди которых находятся Hadoop MapReduce и HDFS. 
\paragraph{MapReduce} --- модель программирования для обработки больших объемов данных, впервые опубликованная\cite{googlemr} Google в 2004 году. При данном подходе логика программы реализуется в функциях \textit{map}, которая преобразует пары ключ/значение в набор промежуточных пар ключ/значение, и \textit{reduce}, которая обрабатывает все значения связанные с одним промежуточным ключом \ref{eq:mapred}.
\begin{equation}\label{eq:mapred}
\begin{split}
map:\langle key_{in}, value_{in}\rangle\rightarrow\langle key_{int}, value_{int}\rangle^{*} \\
reduce:\langle key_{int}, value_{int}^{+}\rangle\rightarrow\langle key_{out}, value_{out}\rangle^{*}
\end{split}
\end{equation}
Написанная таким образом программа может автоматически параллельно выполняться на кластере машин, программное обеспечение которых брало бы на себя распределение данных, управление выполнением задач, поддержку отказов и управление взаимодействием между узлами кластера. 
\paragraph{HDFS (Hadoop Distributed File System)} --- распределенная система хранения данных, основанная на модели GFS\cite{gfs} --- распределенной файловой системы используемой Google. HDFS предназначена для:
\begin{itemize}
 \item больших файлов --- имеются ввиду файлы от нескольких сотен мегабайт до нескольких терабайт;
 \item потокового доступа к данным --- предполагается что данные записываются один раз, и программа в процессе работы использует большую часть из записанного набора данных;
 \item дешевого оборудования --- риск отказа оборудования достаточно высок.
\end{itemize}
Файл в HDFS представляет собой последовательность достаточно больших блоков (по умолчанию 64Mb), которые в нескольких экземплярах (обычно используется 3 реплики) хранятся на различных узлах ---\textit{DataNode}. Последовательность блоков в файле и их расположение управляется через \textit{NameNode}. Таким образом нагрузка на передачу и запись данных распределяется между набором \textit{DataNode}, а структура папок и файлов находится в \textit{NameNode}.

\subsection{Этапы сборки в Nutch}
Nutch является средством инкрементальной сборки\cite{nutchcase}, на каждом этапе выполняются следующие действия:
\begin{itemize}
 \item \textit{inject} --- добавление списка URL в базу ссылок \textit{crawldb} (используется при инициализации сборки или при добавлении новых доменов);
 \item \textit{generate} --- из \textit{crawldb} выбирается фиксированное число ссылок для их последующего скачивания;
 \item \textit{fetch} --- скачивание документов по выбранным ссылкам;
 \item \textit{parse} --- разбор документов и выделение их них ссылок;
 \item \textit{invertlinks} --- обновление базы обратных ссылок;
 \item \textit{index} --- создание индекса по сегменту;
 \item \textit{merge index} --- объединение индекса по сегменту с основным;
 \item \textit{update} --- обновление \textit{crawldb}
\end{itemize}

\subsection{Система Плагинов}
Особого внимания заслуживает система плагинов в Nutch, через которую реализована основная функциональность. Плагины выполняют разбор документов, индексацию, поиск, ранжирование, фильтрацию ссылок и.т.д.

Каждый плагин предоставляет одно или несколько \textit{расширений} (\textit{extensions}) для \textit{точек расширений} (\textit{extension points}), причем сами по себе точки расширений определены в плагине.
