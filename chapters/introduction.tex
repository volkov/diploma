\chapter{Введение} 
\epigraph{— В то время, когда наши корабли бороздят просторы
Вселенной…}{} На текущем этапе развития, когда общество осуществляет переход от
постиндустриальной эпохи к информационной, требования к системам хранения и
обработки информации непрерывно растут. Традиционные подходы не справляются с
ростом количетва данных. Трудно оценить общий объем данных, однако, по оценкам
IDC (International Data Corporation) в данный момент хранится порядка
$1.8\cdot10^{21}$ байт, что в 10 раз больше чем в 2006 году.

К значительному количеству данных можно получить доступ через Всемирную Паутину (www). При таких объемах остро стоит задача организации эффективного поиска. Уже в 2009 году Google Search обработал
более 109,5 миллионов сайтов, и более $10^{12}$ уникальных url. На данный момент их индекс содержит $40\cdot10^{9}$ документов.

\section{Web Search} 
\textbf{Поисковая система} --- система, разработанная для
поиска информации в www. Результаты поиска которой, как правило, представлены в
виде списка ``попадений''\fixme{Не понятно что такое попадения}. Информация может состоять из веб страниц,  изображений,
мультимедийной информации. Одной из первых поисковых систем стал
проект Archie, разработанный в 1990 году студентами McGill University. Программа скачивала
списки файлов с открытых ftp серверов, и добавляла их в базу с возможностью
поиска по названию. %Дальнейшим 

Поисковая система состоит из трех основных компонент: 
\begin{itemize}
 \item поисковый робот --- программа, предназначенная для перебора докуметнов и
занесения данных о них в базу. 
 \item индексатор --- программа, создающая на основе полученных с помощью робота данных индекс.
 \item поисковик --- программа, осуществляющая поиск в полученном индексе на основе поискового запроса.
\end{itemize} В условиях постоянно расширяющегося и изменяющегося www, непрерывно
возрастают требования к поисковым системам. 

\paragraph{Системы общего поиска} нацелены на охват большей части данных
доступных в www. Такие системы предназначены для поиска наиболее релевантных
документов относящихся к объекту поиска. 
\paragraph{Системы тематического поиска} более разнообразны, и требования к ним более специфичны.
 Например Google Microblogging Search Engine, ориентированный на поиск по записям в микроблогах,
где крайне важна задержка между созданием записи, и ее попадением в индекс.

\section{Поиск по новостям}

Основные источники новостей в www --- это электронные СМИ и блоги. По данным
liveinternet на 2008 год, рунет насчитывает 4392 сайта СМИ. 
\fixme{Дополнить данными по нашему проекту. Посмотреть, сколько мы считываем URL'ов по RSS}
Очевидно, за прошедшее время количество таких сайтов значительно увеличилось. За сутки каждое
из подобный изданий публикует до 100 документов(lenta.ru). Таким образом, 
можно говорить о десятках миллионов создаваемых документов в год.

Под новостью понимается документ содержащий текст, заголовок и дату. Для СМИ и
блогов характерно:
\begin{itemize} 
 \item большое количество посторонних страниц, не содержащих новостей;
 \item схожая структура (как именования url, так и самого html);
 \item наличие rss ленты.
\end{itemize}

К новостным поисковым системам предъявляют следущие требования:
\begin{itemize} 
\item минимальное время между публикацией статьи на новостном ресурсе и ее 
    предоставление в поисковой выдаче;
\item поик должен осуществлять не по всей HTML--странице, а только по ее 
    существенным частям. 
\end{itemize}

\section{Задача}
Конечной целью работы является создание системы способной эффективно индексировать новости в рунете.

\fixme{перенести в конец}
\section{Результаты}
\begin{itemize}
 \item Проанализированы различные open source поисковые роботы (DataparkSearch, AppSeek, mnlGoSearch, Nutch, Hounder, Heritix) и выбран в качестве основы Nutch.
 \item Изменено поведение ядра Nutch для более эффективной работы с индексом большого объема.
 \item Проанализированы различные key-value хранилища (Memcached, MongoDb, Project Voldemort, Tokyo Cabinet) и выбрано MongoDb в качестве хранилища для системы удаления дубликатов из индекса
 \item Разработан и реализован плагин к Nutch для раннего удаления дубликатов
 \item Разработан и реализован плагин для более эффективного ранжирования ссылок для новостных сайтов
 \item Разработана и реализована система для автоматического создания url фильтров
 \item Измененная система протестирована на реальных данных.
\end{itemize}

\chapter{Обзор средств}
\section{Сравнение open source поисковых роботов}
Существует достаточно много open source поисковых роботов.
\paragraph{Метрики}
\begin{itemize}
 \item язык
 \item поддержка robots.txt
 \item распределенность системы
 \item тип хранения индекса
 \item тип хранилища url
\end{itemize}
\paragraph{Роботы}
\begin{itemize}
 \item DataparkSearch --- поисковая система разработаная для поиска по локалным файлам, группам сайтов и интранету
 \item AspSeek --- поисковая система оптимизированная для работы с многими сайтами, и средней загрузкой --- до нескольких миллионов страниц
 \item mnoGoSearch --- yet another crawler in C
 \item Nutch --- поисковая система основанная на Lucene
 \item Hounder --- поисковая система онованная на nutch
 \item Heritix --- еще одна java поисковая система
\end{itemize}
\paragraph{Сравнение}
\begin{table}[h]
\caption{\label{tab:crawlers}Сравнение поисковых роботов.}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Название & Язык & Распределенность & robots.txt & Индекс & Хранилище url & Количество документов\\
\hline
DataparkSearch & C & ~ & + & SQL database/собственный формат & SQL database & $10^{6}$\\
\hline
AspSeek & C++ & ?? & + & SQL database & SQL database & $10^{6}$\\
\hline
\end{tabular}
\end{center}
\end{table}
\subsection{Описание роботов}
\paragraph{DataparkSearch}\footnote{http://www.dataparksearch.org/} --- предназначен для работы с небольшой группой сайтов или интранета, написан на C. Состоит из двух частей --- индексатора и CGI фронтенда. DataparkSearch отделился в 2003 году от mnoGoSearch. Имеет встроенные парсеры для html, xml, есть возможность написания собственных парсеров для других форматов. Данные по ссылкам хрянятся в SQL базе данных. Можно запустить сразу несколько процессов индексации работающих с одной базой. Данные по документам могут храниться как в бд, так и в собственном формате на диске (cache mode), который эффективно работает с несколькими миллионами документов.
\paragraph{AspSeek}\footnote{http://www.aspseek.org/} --- поисковая система написанная на C++ и оптимизированная для работы с множеством сайтов. Состоит из индексирующего робота, поискового демона и CGI фронтенда. Данные поискового сервера хранятся в SQL базе данных и бинарных файлах (delta files), рассчитан для работы с несколькими миллионами документов.


